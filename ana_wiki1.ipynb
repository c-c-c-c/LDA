{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import MeCab\n",
    "import os\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './jawiki_data/'  # 用意したデータのあるdirectory\n",
    "infile_base = root_dir + 'jawiki-latest-pages-articles-'\n",
    "N_ARTICLES = 551  # 記事数\n",
    "MARK_ARTICLE = '=' * 20  # 記事ごとの分割マーク\n",
    "sample_ratio = 0.3  # 時間節約のため全記事の30%のみ使用\n",
    "\n",
    "\n",
    "SUFFIX_SPLIT = '_split_v2.txt'\n",
    "SUFFIX_TOPIC = '_topicid_v2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ja_tokenize(text):\n",
    "#     mecab = MeCab.Tagger(\"-Ochasen -d /opt/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
    "    mecab.parse('')\n",
    "    lines = text.split('\\n')\n",
    "    results = []\n",
    "    for s in lines:\n",
    "        node = mecab.parseToNode(s)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            if features[0] != 'BOS/EOS':\n",
    "                w = features[-3]  # word_base\n",
    "                p = features[0]  # part of speech\n",
    "                if p in ['名詞', '形容詞', '動詞']:\n",
    "                    results.append(w)\n",
    "            node = node.next\n",
    "        results.append('\\n')\n",
    "    return results\n",
    "\n",
    "def to_article_words(nums):\n",
    "    fname_topicid =\\\n",
    "        infile_base + '%03d-%03d' % (nums[0], nums[-1]) + SUFFIX_TOPIC\n",
    "    g = open(fname_topicid, 'w')\n",
    "    for i in nums:\n",
    "        path = infile_base + '%03d' % i + '.txt'\n",
    "        print(path)\n",
    "        file_split = path.replace('.txt', SUFFIX_SPLIT)\n",
    "\n",
    "        text = open(path, 'r').read()\n",
    "        text = text.lower()\n",
    "        for word in ['file:', 'ファイル:', 'image:', '画像:']:\n",
    "            text = text.replace('[[' + word, word)\n",
    "        text = text.replace(']]', '').replace(':', ' ')\n",
    "        for j, s in enumerate(text.split('\\n\\n[[')):\n",
    "            words = ja_tokenize(s)\n",
    "            if j == 0:\n",
    "                wt = ' '.join(words)\n",
    "            else:\n",
    "                wt += MARK_ARTICLE + ' '.join(words)\n",
    "            g.write('%d-%d: %s\\n' % (i, j+1, s.split('\\n')[0].replace('[', '').strip()))\n",
    "        open(file_split, 'w').write(wt)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jawiki_data/jawiki-latest-pages-articles-001.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-002.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-003.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-004.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-005.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-006.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-007.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-008.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-009.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-010.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-011.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-012.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-013.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-014.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-015.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-016.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-017.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-018.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-019.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-020.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-021.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-022.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-023.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-024.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-025.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-026.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-027.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-028.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-029.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-030.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-031.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-032.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-033.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-034.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-035.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-036.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-037.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-038.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-039.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-040.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-041.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-042.txt\n",
      "./jawiki_data/jawiki-latest-pages-articles-043.txt\n"
     ]
    }
   ],
   "source": [
    "# 記事情報を単語情報に加工\n",
    "nums = range(1, N_ARTICLES + 1)\n",
    "to_article_words(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記事ごとの単語情報を取得\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "splitfiles = sorted(glob.glob(root_dir + '*' + SUFFIX_SPLIT))\n",
    "#print(len(splitfiles), splitfiles)\n",
    "\n",
    "docs = []\n",
    "j = 0\n",
    "g = open(infile_base + 'resample_docid_v2.txt', 'w')\n",
    "for i, file_split in enumerate(splitfiles):\n",
    "    print(file_split)\n",
    "    texts0 = open(file_split, 'r').read().split(MARK_ARTICLE)\n",
    "    n = len(texts0)\n",
    "    indices = sorted(random.sample(range(n), int(sample_ratio * n)))\n",
    "    for idx in indices:\n",
    "        text = texts0[idx]\n",
    "        docs.append(text.split())\n",
    "        g.write('%d: %d-%d\\n' % (j, i+1, idx+1))\n",
    "        j += 1\n",
    "#     break\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記事ごとの単語情報から辞書を作成\n",
    "if not os.path.exists('./jawiki_model'):\n",
    "    os.mkdir('./jawiki_model')\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "dictionary.save_as_text('./jawiki_model/jawiki_wordid_resample_no-filtered_v2.txt')\n",
    "\n",
    "# あまり重要でない単語を除去\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.3, keep_n=None)\n",
    "dictionary.save_as_text('./jawiki_model/jawiki_wordid_resample_v2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記事ごとの単語ベースの情報（コーパス）を作成\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "gensim.corpora.MmCorpus.serialize('./jawiki_model/jawiki_bow_resample_v2.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパスをTFIDF化\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "with open('./jawiki_model/jawiki_tfidf_resample_v2.dump', mode='wb') as f:\n",
    "    pickle.dump(corpus_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記で作成したコーパスを使い、LDAモデルで最適化する。\n",
    "NUM_TOPICS = 100  # トピック数は100個とした\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load_from_text('./jawiki_model/jawiki_wordid_resample_v2.txt')\n",
    "with open('./jawiki_model/jawiki_tfidf_resample_v2.dump', mode='rb') as f:\n",
    "     corpus_tfidf = pickle.load(f)\n",
    "\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(corpus=corpus_tfidf, id2word=dictionary,\n",
    "                                               num_topics=NUM_TOPICS, workers=3, minimum_probability=0.001,\n",
    "                                               passes=20, chunksize=10000)\n",
    "lda.save('./jawiki_model/jawiki_lda_v2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
